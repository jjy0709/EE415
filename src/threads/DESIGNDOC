            +--------------------+
            |        EE 415      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Hangyeol Kim (khk070623@kaist.ac.kr)
Jiyeong Jeong (jjy0709@kaist.ac.kr)

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

We implemented Alarm Clock, Priority Scheduling, Advanced Scheduler and Scheduling Latency Measurement.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

- struct thread
int64_t wake_time: ticks left until the time thread has to wake up

-struct list block_list: block된 thread의 list

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

It calls thread_sleep with argument ticks.
Then thread_sleep sets current thread's wake time to ticks and 
block the thread, push it to block list.
At each timer tick, check_block function decreases all wake_time of threads in block list.
If wake_time becomes 0, unblock the thread. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

Timer interrupt에 sleep중인 thred를 깨우기 위해서는 all thread list에서 모든 thread를 확인하혀야 했다.
여기에 걸리는 시간을 최소화하기 위해서 sleep 상태인 Thread를 구분하기 위한 block list를 만들어서
모든 thread를 확인하지 않고 blocked인 thread만 확인하도록 하였는데, 이를 통해 timer interrupt handler에서
소요되는 시간을 최소화하였다.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

When thread_sleep is called, we disabled interrupt before thread's wake_time is set and thread is blocked.
By this, we can avoid race conditions.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

thread_sleep이 atomic하게 작동하기 때문에 timer_sleep 도중에 timer interrupt가 발생하여도 thread의 awake 시간에 영향을 주지
않는다.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Block list를 만들지 않으면 all list 안의 모든 thread의 status를 하나하나 확인하고 비교하여야 하는데
block list를 만들어서 blocked 상태의 thread만 바로 확인할 수 있어 thread를 awake 해야하는지 판단하는데
걸리는 시간을 최소화할 수 있었다.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread
{
    int original_priority;  thread의 원래 priority
    int surface_priority;   thread의 현재 priority(priority donation 등을 통해 변할 수 있는 priority)
    
    struct lock* waiting;   thread가 현재 released되길 기다리고 있는 lock
    struct list holdings;   thread가 현재 acquire한 lock들의 list
}

struct lock
{
    struct list_elem elem;  lock을 struct list에 넣기 위한 struct member
}

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---------------------------------------------------------------------------------------------------------------
- thread 1 acquire lock 1
thread1 {
    waiting: NULL
    holdings: [lock 1]
    original_priority: 1
    surface_priority: 1
}

- thread 2 acquire lock 2
thread1 {                       thread2 {
    waiting: NULL                   waiting: NULL
    holdings: [lock 1]              holdings: [lock 2]
    original_priority: 1            original_priority: 3
    surface_priority: 1             surface_priority: 3
}

-thread 2 blocked by lock 1
thread1 {                       thread2 {                       
    waiting: NULL                   waiting: lock 1             
    holdings: [lock 1]              holdings: [lock 2]          
    original_priority: 1            original_priority: 3        
    surface_priority: 1             surface_priority: 3         
}

-donation by thread 2 and thread 3 created
thread1 {                       thread2 {                       thread3 {
    waiting: NULL                   waiting: lock 1                 waiting: NULL
    holdings: [lock 1]              holdings: [lock 2]              holdings: []
    original_priority: 1            original_priority: 3            original_priority: 5
    surface_priority: 3             surface_priority: 3             surface_priority: 5
}

-thread 3 blocked by lock 2
thread1 {                       thread2 {                       thread3 {
    waiting: NULL                   waiting: lock 1                 waiting: lock 2
    holdings: [lock 1]              holdings: [lock 2]              holdings: []
    original_priority: 1            original_priority: 3            original_priority: 5
    surface_priority: 3             surface_priority: 3             surface_priority: 5
}

-donation by thread 3
thread1 {                       thread2 {                       thread3 {
    waiting: NULL                   waiting: lock 1                 waiting: lock 2
    holdings: [lock 1]              holdings: [lock 2]              holdings: []
    original_priority: 1            original_priority: 3            original_priority: 5
    surface_priority: 3             surface_priority: 5             surface_priority: 5
}

-nested donation by thread 3
thread1 {                       thread2 {                       thread3 {
    waiting: NULL                   waiting: lock 1                 waiting: lock 2
    holdings: [lock 1]              holdings: [lock 2]              holdings: []
    original_priority: 1            original_priority: 3            original_priority: 5
    surface_priority: 5             surface_priority: 5             surface_priority: 5
}

-lock 1 released and acquired
thread1 {                       thread2 {                       thread3 {
    waiting: NULL                   waiting: NULL                   waiting: lock 2
    holdings: []                    holdings: [lock1, lock 2]       holdings: []
    original_priority: 1            original_priority: 3            original_priority: 5
    surface_priority: 1             surface_priority: 5             surface_priority: 5
}

-lock 2 released and acquired
thread1 {                       thread2 {                       thread3 {
    waiting: NULL                   waiting: NULL                   waiting: NULL
    holdings: []                    holdings: [lock 1]              holdings: [lock 2]
    original_priority: 1            original_priority: 3            original_priority: 5
    surface_priority: 1             surface_priority: 3             surface_priority: 5
}

---------------------------------------------------------------------------------------------------------------

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

lock, semaphore, condition variable을 implement하는 함수들에서 blocked된 thread를 unblock할 때,
waiters list를 priority 순서에 따라 sort하여 가장 Priority가 높은 thread를 깨우도록 하였다.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

먼저 현재 thread의 waiting을 현재 lock으로 설정한다. 그 후 donate 함수가 실행된다.
donate 함수는 현재 thread가 기다리는 lock의 holder의 surface_priority가 현재 thread의 surface_priority보다 높다면
holder의 surface_priority를 변경하고 holder를 argument로 하여 함수를 재귀로 실행한다.
holder가 lock을 기다리고 있다면 위의 과정을 반복하여 가장 상위의 lock을 가진 thread까지 priority가 donation 된다.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

먼저 lock holder thread의 holdings list에서 lock을 제거한다. 그 후 현재 threaddml holdings에 있는 lock들을 조회하여
다른 lock을 기다리고 있는 가장 높은 priority를 계산한다. 만약 기다리는 thread가 없다면 현재 thread의 surface_priority는
original_priority가 된다. 그 후 lock의 holder를 NULL로 설정하고 sema_up을 실행한다. sema_up에서는 sema waiters를
surface_priority 순으로 정렬한 뒤 가장 높은 priority를 가진 thread를 unblock한다. 현재 thread보다 높은 priority를 
가진 thread가 lock을 기다리고 있었다면 thread_yield 함수를 통해 cpu를 yield할 수 있게 된다.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

donation 함수에서 thread의 surface_priority를 변경하고자 할 때 thread_set_priority()와 충돌하여 race condition이
생길 수 있다. 이를 방지하기 위해 thread_set_priority 함수에서 interrupt를 disable하였다. thread_set_priority와 donation
함수에서 같은 lock을 acquire하도록 하여 이 race condition을 해결할 수 있을 것이다.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

priority donation list를 consider했었으나 이 경우 어떤 lock을 어떤 thread가 기다리고 있는지 확인하는 것이 복잡하고 lock을 release한
후 관련된 lock을 list에서 하나씩 비교하며 제거해야한다는 번거러움이 있다. 현재 thread가 가지고 있는 lock들을 list로 접근할 수 있으면 waiters에도
접근할 수 있어 다른 thread의 priority에도 접근할 수 있고 각 lock마다의 waiters를 통해 lock_release 후 다음으로 필요한 Priority를 비교적
편하게 가져올 수 있기 때문에 holdings와 waiting 만을 추가한 이 구조를 선택하게 되었다.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In struct thread:
    int nice;
    int cpu;
We added integer 'nice' and 'cpu' in struct thread to keep track of each thread's nice value and recent cpu value.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0   63  61  59      A              
 4      4   0   0   62  61  59      A
 8      8   0   0   61  61  59      B
12      8   4   0   61  60  59      A
16      12  4   0   60  60  59      B
20      12  8   0   60  59  59      A   
24      16  8   0   59  59  59      C
28      16  8   4   59  59  58      B
32      16  12  4   59  58  58      A
36      20  12  4   58  58  58      C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

When there are multiple highest-priority threads, we run it in order so that the thread that has been waiting longest runs first (FIFO). Our code works this way.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

We put all computation of recent_cpu time, load_avg, priority inside the timer interrupt handler.
This may affect the performance in a bad way (because the less work inside the interrupt handler, the better), but since our implementation passed all the tests successfully, we thought it was not neccessary to fix or reimplement.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

As I mentioned in C4, we would have found a way to make the computation of load_avg, recent cpu time, and priority outside the timer interrupt.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

We made fixed-point.h inside threads folder and implemented all the fixed-point related arithmetic abstraction.
We decided to implement 8 requisite & distinct functions among the 11 functions inside the 4.4BSD document.
Did so because without this abstraction, writing complex equation related to mlfqs scheduling makes the code hardly legible.

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

We think it was appropriate.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

By completing this work, we got some insight into some aspect of OS design.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

It would be better to give students some information about race conditions that could be exist in OS.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

No

>> Any other comments?